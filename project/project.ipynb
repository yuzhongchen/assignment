{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bJvw-FG9eVk3"
      },
      "outputs": [],
      "source": [
        "# !pip install webvtt-py spacy sentence-transformers qdrant-client bertopic gradio ffmpeg-python\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v1A91-h0eVk7"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "from webvtt import WebVTT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h8o1bB-fFfQ",
        "outputId": "2ff663a5-3e8c-43eb-8317-50a52225b5ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6970, 'caption segments loaded.')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "def time_to_seconds(timestamp):\n",
        "    h, m, s = timestamp.split(':')\n",
        "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
        "\n",
        "captions_data = []  # List of dicts: {'video_id', 'start', 'end', 'text'}\n",
        "for video_dir in sorted(os.listdir(\"./data/videos\")):\n",
        "    video_path = os.path.join(\"./data/videos\", video_dir)\n",
        "    if not os.path.isdir(video_path):\n",
        "        continue\n",
        "    # Find English VTT file\n",
        "    vtt_files = glob.glob(os.path.join(video_path, \"*.en.vtt\"))\n",
        "    if not vtt_files:\n",
        "        continue\n",
        "    vtt_path = vtt_files[0]\n",
        "    # Parse captions\n",
        "    for caption in WebVTT().read(vtt_path):\n",
        "        text = caption.text.strip().replace(\"\\n\", \" \")\n",
        "        if not text:\n",
        "            continue\n",
        "        start = time_to_seconds(caption.start)\n",
        "        end = time_to_seconds(caption.end)\n",
        "        captions_data.append({\"video_id\": video_dir, \"start\": start, \"end\": end, \"text\": text})\n",
        "\n",
        "# Sort by video and time\n",
        "captions_data.sort(key=lambda x: (x['video_id'], x['start']))\n",
        "\n",
        "# Deduplicate consecutive segments\n",
        "filtered_caps = []\n",
        "prev_text = None\n",
        "prev_vid = None\n",
        "for cap in captions_data:\n",
        "    if cap[\"video_id\"] != prev_vid:\n",
        "        prev_text = None  # reset at new video\n",
        "        prev_vid = cap[\"video_id\"]\n",
        "    if cap[\"text\"] != prev_text:\n",
        "        filtered_caps.append(cap)\n",
        "    prev_text = cap[\"text\"]\n",
        "captions_data = filtered_caps\n",
        "\n",
        "len(captions_data), \"caption segments loaded.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "# !cp -r /content/drive/MyDrive/data /content"
      ],
      "metadata": {
        "id": "4Eo8YIgGUux5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E4ppJT6flEM",
        "outputId": "b006bf55-372f-4032-8997-0d366680f8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 8 merged sentences from captions.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "from itertools import groupby\n",
        "\n",
        "sentences = []  # List of dicts: {'video_id', 'start', 'end', 'sentence'}\n",
        "for video_id, group in groupby(captions_data, key=lambda x: x['video_id']):\n",
        "    group = list(group)\n",
        "    combined_text = \" \".join([seg[\"text\"] for seg in group])\n",
        "    doc = nlp(combined_text)\n",
        "    # Compute cumulative character lengths for segment boundaries\n",
        "    cum_lengths = [0]\n",
        "    for seg in group:\n",
        "        cum_lengths.append(cum_lengths[-1] + len(seg[\"text\"]) + 1)\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.strip()\n",
        "        if not sent_text:\n",
        "            continue\n",
        "        start_char = sent.start_char\n",
        "        end_char = sent.end_char\n",
        "        # Find segment indices containing sentence start/end\n",
        "        start_idx = max(i for i, length in enumerate(cum_lengths) if length <= start_char)\n",
        "        end_idx = max(i for i, length in enumerate(cum_lengths) if length < end_char)\n",
        "        start_time = group[start_idx][\"start\"]\n",
        "        end_time = group[end_idx][\"end\"]\n",
        "        sentences.append({\n",
        "            \"video_id\": video_id,\n",
        "            \"start\": start_time,\n",
        "            \"end\": end_time,\n",
        "            \"sentence\": sent_text\n",
        "        })\n",
        "\n",
        "print(f\"Created {len(sentences)} merged sentences from captions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1b1ccd2540224e0c9c49e37c28cd1bd6",
            "ee8a0c0b1de046fd9773b1b4b615e34f",
            "e0d0e54dfff947efbe9b069bb78b328e",
            "fccd54773bd9482a81a27e202ef9a7ef",
            "6cfaa2d5e28c493c8ba47ce83e408d92",
            "de82aa79fc1943bf868fc70bb1b333b4",
            "570dad6717524b23a5145c8894dd686f",
            "c766a5912999411c83a2da98d65dc973",
            "ccbd8e9754e94a07a2c5bc3423f7446e",
            "a7d50317a1ed40d982a38b26d4141d4e",
            "4e8a1ebe6e8c4e528697f50050721c91"
          ]
        },
        "id": "BPq0UUxIv47m",
        "outputId": "8112cd02-5ea0-4f96-eee2-51b2cddd5269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b1ccd2540224e0c9c49e37c28cd1bd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myc6317\u001b[0m (\u001b[33mproject_hpml_yuzhong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_043232-dfs9lacr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/project_hpml_yuzhong/sentence-transformers/runs/dfs9lacr' target=\"_blank\">checkpoints/model_1</a></strong> to <a href='https://wandb.ai/project_hpml_yuzhong/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/project_hpml_yuzhong/sentence-transformers' target=\"_blank\">https://wandb.ai/project_hpml_yuzhong/sentence-transformers</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/project_hpml_yuzhong/sentence-transformers/runs/dfs9lacr' target=\"_blank\">https://wandb.ai/project_hpml_yuzhong/sentence-transformers/runs/dfs9lacr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [90/90 00:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# !pip install accelerate>=0.21.0\n",
        "# !pip install -U transformers\n",
        "# !pip install peft==0.3.0\n",
        "# !pip install datasets\n",
        "# !pip install \"sentence-transformers[train]\"\n",
        "!pip install accelerate --upgrade\n",
        "!pip install peft --upgrade\n",
        "!pip install transformers --upgrade\n",
        "from itertools import groupby\n",
        "from torch.utils.data import DataLoader\n",
        "import spacy\n",
        "import random\n",
        "# Import the datasets library and the Dataset class\n",
        "# !pip install --upgrade sentence-transformers datasets\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "def windowize_sentence(sentence, window_size=50, stride=50):\n",
        "    tokens = sentence.split()\n",
        "    spans = []\n",
        "    # slide a window of up to 50 words (no overlap, since stride=window_size)\n",
        "    for start in range(0, max(1, len(tokens) - window_size + 1), stride):\n",
        "        spans.append(\" \".join(tokens[start:start + window_size]))\n",
        "    if not spans:\n",
        "        spans = [\" \".join(tokens)]\n",
        "    return spans\n",
        "\n",
        "processed = []\n",
        "for vid, group in groupby(sentences, key=lambda x: x['video_id']):\n",
        "    group = list(group)\n",
        "    long_para = group[0]['sentence']\n",
        "    for sent in nlp(long_para).sents:\n",
        "        # further split any spaCy sentence over 50 words\n",
        "        for chunk in windowize_sentence(sent.text, window_size=50, stride=50):\n",
        "            processed.append({\n",
        "                \"video_id\": vid,\n",
        "                \"sentence\": chunk.strip()\n",
        "            })\n",
        "\n",
        "train_examples = []\n",
        "for vid, group in groupby(processed, key=lambda x: x['video_id']):\n",
        "    grp = list(group)\n",
        "    for i in range(len(grp) - 1):\n",
        "        s1, s2 = grp[i]['sentence'], grp[i+1]['sentence']\n",
        "        if not s1 or not s2:\n",
        "            continue\n",
        "        # import here, where InputExample is used\n",
        "        from sentence_transformers import InputExample\n",
        "        train_examples.append(InputExample(texts=[s1, s2]))\n",
        "\n",
        "# 4. Shuffle, batch, and fine‐tune\n",
        "random.shuffle(train_examples)\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "# import here, where SentenceTransformer is used\n",
        "from sentence_transformers import SentenceTransformer, losses\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=1,\n",
        "    warmup_steps=100\n",
        ")\n",
        "\n",
        "# 5. Save your fine‐tuned model\n",
        "model.save(\"fine_tuned_minilm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxz7RvNvTGno",
        "outputId": "6622217b-10d9-4b9b-b2c6-e38294ca6fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
            "Encoded sentences to vectors of shape: (1435, 384)\n"
          ]
        }
      ],
      "source": [
        "# !pip install peft --upgrade\n",
        "from peft import PeftModelForFeatureExtraction\n",
        "sent_texts = [\n",
        "    chunk\n",
        "    for s in sentences\n",
        "    for sent in nlp(s[\"sentence\"]).sents  # Split into spaCy sentences\n",
        "    for chunk in windowize_sentence(sent.text)  # Further split into windowed chunks\n",
        "]\n",
        "embeddings = model.encode(sent_texts, convert_to_numpy=True)\n",
        "print(\"Encoded sentences to vectors of shape:\", embeddings.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPniVGIaTGnp",
        "outputId": "bc2b3ad6-9689-4440-b1e5-9dae9ce415d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1435\n",
            "   Topic  Count                              Name  \\\n",
            "0     -1    495                   -1_uh_the_of_to   \n",
            "1      0    178        0_feature_map_depth_filter   \n",
            "2      1     87  1_layers_layer_convolutional_are   \n",
            "3      2     58             2_image_images_uh_car   \n",
            "4      3     50          3_signal_was_strength_of   \n",
            "\n",
            "                                      Representation  \\\n",
            "0     [uh, the, of, to, is, and, this, that, in, we]   \n",
            "1  [feature, map, depth, filter, the, output, inp...   \n",
            "2  [layers, layer, convolutional, are, the, uh, s...   \n",
            "3  [image, images, uh, car, in, um, of, that, you...   \n",
            "4  [signal, was, strength, of, uh, the, this, wer...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [uh to be called the radar problem and uh we'l...  \n",
            "1  [input feature map of depth map of depth map o...  \n",
            "2  [in the fully connected dense layer fully conn...  \n",
            "3  [in they are uh positioned at some point in th...  \n",
            "4  [the radar problem High signal strength low si...  \n"
          ]
        }
      ],
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "print(len(sent_texts))\n",
        "topics, probs = topic_model.fit_transform(sent_texts)\n",
        "print(topic_model.get_topic_info().head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client\n",
        "import os\n",
        "import time\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
        "\n",
        "\n",
        "# Set Qdrant storage path to be within the current working directory\n",
        "os.environ[\"QDRANT_STORAGE_PATH\"] = \"./qdrant_storage\"\n",
        "\n",
        "# Start the Qdrant server in the background\n",
        "client = QdrantClient(host=\"localhost\", port=6333)\n",
        "print(\"Qdrant server started\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Hn3cZLhGBA",
        "outputId": "1cb9f1ec-f9b8-4fd2-b638-4f53f07e3836"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.11/dist-packages (1.14.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.0.2)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (5.29.4)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Qdrant server started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uTtxN7hTGnp",
        "outputId": "7cb592d0-b262-465a-a6e7-bc6038e7eb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qdrant-client[fastembed] in /usr/local/lib/python3.11/dist-packages (1.14.2)\n",
            "Collecting fastembed==0.6.1 (from qdrant-client[fastembed])\n",
            "  Downloading fastembed-0.6.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.0.2)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.10.1)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (5.29.4)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.11.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client[fastembed]) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (0.30.2)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<6.0.0,>=4.1.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (11.2.1)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed==0.6.1->qdrant-client[fastembed])\n",
            "  Downloading py_rust_stemmers-0.1.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.11/dist-packages (from fastembed==0.6.1->qdrant-client[fastembed]) (4.67.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client[fastembed]) (0.4.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.6.1->qdrant-client[fastembed]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.6.1->qdrant-client[fastembed]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.6.1->qdrant-client[fastembed]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed==0.6.1->qdrant-client[fastembed]) (6.0.2)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed]) (1.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->fastembed==0.6.1->qdrant-client[fastembed]) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client[fastembed]) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed==0.6.1->qdrant-client[fastembed]) (1.3.0)\n",
            "Downloading fastembed-0.6.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_rust_stemmers-0.1.5-cp311-cp311-manylinux_2_28_x86_64.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py-rust-stemmers, mmh3, loguru, humanfriendly, coloredlogs, onnxruntime, fastembed\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.6.1 humanfriendly-10.0 loguru-0.7.3 mmh3-5.1.0 onnxruntime-1.22.0 py-rust-stemmers-0.1.5\n",
            "Successfully stored 8 vectors!\n"
          ]
        }
      ],
      "source": [
        "# First install required packages\n",
        "!pip install -U qdrant-client[fastembed]\n",
        "\n",
        "from qdrant_client import QdrantClient, models\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Start Qdrant in standalone mode (no Docker needed)\n",
        "client = QdrantClient(\n",
        "    location=\":memory:\",  # Use in-memory storage\n",
        "    prefer_grpc=False     # Disable gRPC for Colab compatibility\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "client.recreate_collection(\n",
        "    collection_name=\"video_captions\",\n",
        "    vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "# Generate sample embeddings (replace with your actual data)\n",
        "embeddings = np.random.rand(len(sentences), 384)\n",
        "\n",
        "# Prepare and insert points\n",
        "points = [\n",
        "    models.PointStruct(\n",
        "        id=i,\n",
        "        vector=embedding.tolist(),\n",
        "        payload={\n",
        "            \"video_id\": sent[\"video_id\"],\n",
        "            \"start\": sent[\"start\"],\n",
        "            \"end\": sent[\"end\"],\n",
        "            \"sentence\": sent[\"sentence\"]\n",
        "        }\n",
        "    )\n",
        "    for i, (sent, embedding) in enumerate(zip(sentences, embeddings))\n",
        "]\n",
        "\n",
        "# Insert data\n",
        "client.upsert(\n",
        "    collection_name=\"video_captions\",\n",
        "    points=points\n",
        ")\n",
        "\n",
        "print(f\"Successfully stored {len(points)} vectors!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbEuewo-TGnq",
        "outputId": "cb830568-067e-4e26-9bed-7feffc5483fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top answer: \"we have seen we have seen earlier the regression problem uh where earlier the regression problem uh where earlier the regression problem uh where we have effectively to model a we have effectively to model a we have effectively to model a conditional probability distribution at conditional probability distribution at conditional probability distribution at the output of our predictor now we the output of our predictor now we the output of our predictor now we actually switching to a new task it's a actually switching to a new task it's a actually switching to a new task it's a classification task where again we will classification task where again we will classification task where again we will solve this task using The Vaping block solve this task using The Vaping block solve this task using The Vaping block diagram again we will need to model our diagram again we will need to model our diagram again we will need to model our predictor with a conditional probability predictor with a conditional probability predictor with a conditional probability distribution but in classification our distribution but in classification our distribution but in classification our Target variables are distinct and Target variables are distinct and Target variables are distinct and discrete random variables rather than discrete random variables rather than discrete random variables rather than continues so in this kind of setting continues so in this kind of setting continues so in this kind of setting I'll motivate the classification uh task I'll motivate the classification uh task I'll motivate the classification uh task with a simple use case is actually going with a simple use case is actually going with a simple use case is actually going to be called the radar problem and uh to be called the radar problem and uh to be called the radar problem and uh we'll uh that's what we will start with we'll uh that's what we will start with we'll uh that's what we will start with uh uh uh next in this uh setting the use case the next in this uh setting the use case the next in this uh setting the use case the application we will see uh is a application we will see uh is a application we will see uh is a well-known application back in the uh uh well-known application back in the uh uh well-known application back in the uh uh Second World War Second World War Second World War uh the uh Battle of England uh the uh Battle of England uh the uh Battle of England uh was W primarily from uh the erection uh was W primarily from uh the erection uh was W primarily from uh the erection of these of these of these towers uh this was actually called the towers uh this was actually called the towers uh this was actually called the radar Towers uh whose job is was to radar Towers uh whose job is was to radar Towers uh whose job is was to transmit a transmit a transmit a signal uh towards the um France where uh signal uh towards the um France where uh signal uh towards the um France where uh from France the Nazi airplanes were from France the Nazi airplanes were from France the Nazi airplanes were coming in uh to bomb coming in uh to bomb coming in uh to bomb London and uh the every time that the London and uh the every time that the London and uh the every time that the this uh waveform was impinging into some this uh waveform was impinging into some this uh waveform was impinging into some large object on the sky like a plane it large object on the sky like a plane it large object on the sky like a plane it was returning back uh into what we call was returning back uh into what we call was returning back uh into what we call the radar receiver and uh there was kind of a receiver and uh there was kind of a human operator over there in uh uh on human operator over there in uh uh on human operator over there in uh uh on the on each kind of Tower with an access the on each kind of Tower with an access the on each kind of Tower with an access to a uh a kind of a telephone device to a uh a kind of a telephone device to a uh a kind of a telephone device over there and every time that there was over there and every time that there was over there and every time that there was um a strong return um a strong return um a strong return uh of a strong signal that was received uh of a strong signal that was received uh of a strong signal that was received in the radar kind of a receiver antenna in the radar kind of a receiver antenna in the radar kind of a receiver antenna it was uh he was um calling London and it was uh he was um calling London and it was uh he was um calling London and millions of people were well the sirens millions of people were well the sirens millions of people were well the sirens were sounding and millions of people were sounding and millions of people were sounding and millions of people were actually running uh to the tubes uh were actually running uh to the tubes uh were actually running uh to the tubes uh stations to tube stations to uh uh save stations to tube stations to uh uh save stations to tube stations to uh uh save their lives that was the application and their lives that was the application and their lives that was the application and um this application is evidently present um this application is evidently present um this application is evidently present in any modern car today that has uh in any modern car today that has uh in any modern car today that has uh exactly the the same ability to um have exactly the the same ability to um have exactly the the same ability to um have Raiders located in the uh let's say in Raiders located in the uh let's say in Raiders located in the uh let's say in the front uh of the car to send exactly the front uh of the car to send exactly the front uh of the car to send exactly the same signals uh and every time that the same signals uh and every time that the same signals uh and every time that you have a feature in your car that it's you have a feature in your car that it's you have a feature in your car that it's automatic um what is called uh automatic automatic um what is called uh automatic automatic um what is called uh automatic uh distance keeping uh to the vehicle in uh distance keeping uh to the vehicle in uh distance keeping uh to the vehicle in front of you that is exactly the same front of you that is exactly the same front of you that is exactly the same thing um it was returning back that thing um it was returning back that thing um it was returning back that Reflection from the car in front and uh Reflection from the car in front and uh Reflection from the car in front and uh controller is actually trying to keep controller is actually trying to keep controller is actually trying to keep the the velocities between your car and the the velocities between your car and the the velocities between your car and the car in front of you constant and the car in front of you constant and the car in front of you constant and therefore maintain a desired distance therefore maintain a desired distance therefore maintain a desired distance between the two cars but anyway I would between the two cars but anyway I would between the two cars but anyway I would motivate it with this um sort of uh motivate it with this um sort of uh motivate it with this um sort of uh application which is the uh World War II application which is the uh World War II application which is the uh World War II application because also back then a lot application because also back then a lot application because also back then a lot of the terminology that we will cover of the terminology that we will cover of the terminology that we will cover today it was also uh first today it was also uh first today it was also uh first invented uh and uh just to see exactly invented uh and uh just to see exactly invented uh and uh just to see exactly what is actually happening here we have what is actually happening here we have what is actually happening here we have a some kind of a time a some kind of a time a some kind of a time plot of a quantity that will be calling plot of a quantity that will be calling plot of a quantity that will be calling X and it will be designating for us the X and it will be designating for us the X and it will be designating for us the so-call so-call so-call signal strength of or received signal strength strength of or received signal strength or power and very simplistically we have or power and very simplistically we have or power and very simplistically we have some you know some some you know some some you know some fluctuation received power when we have fluctuation received power when we have fluctuation received power when we have a return a strong return from a plane a return a strong return from a plane a return a strong return from a plane and in nights where we don't the attacks and in nights where we don't the attacks and in nights where we don't the attacks were usually during the night were usually during the night were usually during the night uh that uh when we have no return we uh that uh when we have no return we uh that uh when we have no return we still have some fluctuating signal power still have some fluctuating signal power still have some fluctuating signal power much smaller than much smaller than much smaller than that um and uh okay our uh receiver uh that um and uh okay our uh receiver uh that um and uh okay our uh receiver uh is going to have just one knob that knob is going to have just one knob that knob is going to have just one knob that knob is going to be called the threshold will is going to be called the threshold will is going to be called the threshold will symbolize it with W it's a scalar value symbolize it with W it's a scalar value symbolize it with W it's a scalar value that it is uh anything that exceeds it that it is uh anything that exceeds it that it is uh anything that exceeds it is going to let's assume that this threshold value let's assume that this threshold value is set to this kind of level everything is set to this kind of level everything is set to this kind of level everything that exceeds it is going to be called that exceeds it is going to be called that exceeds it is going to be called the the the predicted uh that is uh well not predicted uh that is uh well not predicted uh that is uh well not predicted is we're going to sound the predicted is we're going to sound the predicted is we're going to sound the alarm and anything that it is not alarm and anything that it is not alarm and anything that it is not exceeding it uh we are going to not exceeding it uh we are going to not exceeding it uh we are going to not alarm anyone alert anyone and we alarm anyone alert anyone and we alarm anyone alert anyone and we will uh call that uh the no attack case will uh call that uh the no attack case will uh call that uh the no attack case so our problem is um a binary so our problem is um a binary so our problem is um a binary classification problem what we call classification problem what we call classification problem what we call binary because our now Target variable binary because our now Target variable binary because our now Target variable as compared to that regression can only as compared to that regression can only as compared to that regression can only take two values one we will call this take two values one we will call this take two values one we will call this the so-called positive condition the the so-called positive condition the the so-called positive condition the attack is going on and zero we will be attack is going on and zero we will be attack is going on and zero we will be calling this the So-Cal calling this the So-Cal calling this the So-Cal negative uh condition and uh our uh job negative uh condition and uh our uh job negative uh condition and uh our uh job here uh now that we are in uh uh the here uh now that we are in uh uh the here uh now that we are in uh uh the 2020s we are going to solve this problem 2020s we are going to solve this problem 2020s we are going to solve this problem of determining the uh value of that kind of determining the uh value of that kind of determining the uh value of that kind of threshold using kind of uh machine of threshold using kind of uh machine of threshold using kind of uh machine learning approaches so we have uh what learning approaches so we have uh what learning approaches so we have uh what we will do is we will task a a military we will do is we will task a a military we will do is we will task a a military person over here to sit and observe what person over here to sit and observe what person over here to sit and observe what is really happening every single day so is really happening every single day so is really happening every single day so uh first night uh records the signal uh first night uh records the signal uh first night uh records the signal strength X1 and Records also whether strength X1 and Records also whether strength X1 and Records also whether attack has happened or not X2 the same attack has happened or not X2 the same attack has happened or not X2 the same way uh and so on if they survive we will way uh and so on if they survive we will way uh and so on if they survive we will uh keep them if they not we have to uh keep them if they not we have to uh keep them if they not we have to replace them with somebody else uh so XM replace them with somebody else uh so XM replace them with somebody else uh so XM ym that's our let's say we have ym that's our let's say we have ym that's our let's say we have aggregated M examples of um what is has aggregated M examples of um what is has aggregated M examples of um what is has happened at our receiver and as before happened at our receiver and as before happened at our receiver and as before we progress into specifying what is we progress into specifying what is we progress into specifying what is really the uh problem statement I just really the uh problem statement I just really the uh problem statement I just want to U convey some kind of an want to U convey some kind of an want to U convey some kind of an intuitive fashion of what is happening intuitive fashion of what is happening intuitive fashion of what is happening so this was the soal attack signal so this was the soal attack signal so this was the soal attack signal strength and this was the uh no attack uh signal strength the sort of uh attack uh signal strength the sort of uh time series of all these kind of time series of all these kind of time series of all these kind of observations night after night and we observations night after night and we observations night after night and we have um have um have um um the problem to solve is to come up um the problem to solve is to come up um the problem to solve is to come up with an optimal value for the threshold with an optimal value for the threshold with an optimal value for the threshold you can understand that the threshold is you can understand that the threshold is you can understand that the threshold is quite critical in determining whether um quite critical in determining whether um quite critical in determining whether um how the system operates uh if we set the how the system operates uh if we set the how the system operates uh if we set the threshold too high uh this means that we threshold too high uh this means that we threshold too high uh this means that we will be um missing quite a lot of will be um missing quite a lot of will be um missing quite a lot of attacks and evidently people will die if attacks and evidently people will die if attacks and evidently people will die if we set the threshold too low um this we set the threshold too low um this we set the threshold too low um this means that we will be means that we will be means that we will be uh uh waking up uh uh waking up uh uh waking up everyone um everyone um everyone um unnecessarily uh the um first night they unnecessarily uh the um first night they unnecessarily uh the um first night they will believe us and you know uh return will believe us and you know uh return will believe us and you know uh return to their beds uh uh disappointed somehow to their beds uh uh disappointed somehow to their beds uh uh disappointed somehow or relieved uh the second night they or relieved uh the second night they or relieved uh the second night they will still believe us they will go down will still believe us they will go down will still believe us they will go down to the tube um but after the third night to the tube um but after the third night to the tube um but after the third night they will stop believing us and when a they will stop believing us and when a they will stop believing us and when a real ATT happens uh people will uh also real ATT happens uh people will uh also real ATT happens uh people will uh also die so die so die so uh the system will lose complete uh the system will lose complete uh the system will lose complete credibility so the choice of this W is credibility so the choice of this W is credibility so the choice of this W is uh uh is quite critical so our uh uh is quite critical so our uh uh is quite critical so our predictor the Y hat that we have to predictor the Y hat that we have to predictor the Y hat that we have to predict at this moment in time we will predict at this moment in time we will predict at this moment in time we will treat this as a one and zero later will treat this as a one and zero later will treat this as a one and zero later will become uh a number between zero and one become uh a number between zero and one become uh a number between zero and one is also going to return um our prediction predictor is to return um our prediction predictor is going also to return one or zero for the going also to return one or zero for the going also to return one or zero for the positive and positive and positive and negative condition respectively okay so negative condition respectively okay so negative condition respectively okay so that is uh the problem statement how to that is uh the problem statement how to that is uh the problem statement how to set up this W set up this W set up this W optimally um and as you can understand optimally um and as you can understand optimally um and as you can understand uh I can uh just after uh just before in uh I can uh just after uh just before in uh I can uh just after uh just before in the regression kind of problem we had the regression kind of problem we had the regression kind of problem we had introduced uh the probabilistic nature introduced uh the probabilistic nature introduced uh the probabilistic nature of every predictor so what what we will of every predictor so what what we will of every predictor so what what we will do do do now is uh we draw two probability draw two probability distributions and uh let me just draw distributions and uh let me just draw distributions and uh let me just draw this probability distribution let's say this so this is a probability this so this is a probability distribution of X we have here on the x- distribution of X we have here on the x- distribution of X we have here on the x- axis our signal strength axis our signal strength axis our signal strength uh and uh one of them is going to be uh and uh one of them is going to be uh and uh one of them is going to be called the probability of called the probability of called the probability of X and uh when no attack is happening and X and uh when no attack is happening and X and uh when no attack is happening and this is the probability of X when the this is the probability of X when the this is the probability of X when the attack is happening and as you can see attack is happening and as you can see attack is happening and as you can see uh these two prob distributions can be uh these two prob distributions can be uh these two prob distributions can be easily obtained U as histograms by just easily obtained U as histograms by just easily obtained U as histograms by just uh going back to uh our uh data tape uh uh going back to uh our uh data tape uh uh going back to uh our uh data tape uh the data set that we have created the data set that we have created the data set that we have created and recorded and uh select all the rows and recorded and uh select all the rows and recorded and uh select all the rows where the Y Target variable is zero and where the Y Target variable is zero and where the Y Target variable is zero and obtain this histogram and visit all the obtain this histogram and visit all the obtain this histogram and visit all the corresponding rows where the Y is equal corresponding rows where the Y is equal corresponding rows where the Y is equal to one and plot this corresponding to one and plot this corresponding to one and plot this corresponding histogram so in terms of plotting the histogram so in terms of plotting the histogram so in terms of plotting the histograms this is uh definitely a a histograms this is uh definitely a a histograms this is uh definitely a a very I will call it a easy exercise and very I will call it a easy exercise and very I will call it a easy exercise and as you probably notice from the shape of as you probably notice from the shape of as you probably notice from the shape of these histograms we have not really made these histograms we have not really made these histograms we have not really made any uh assumption with respect to U any uh assumption with respect to U any uh assumption with respect to U gaussianity or nothing like that they gaussianity or nothing like that they gaussianity or nothing like that they are definitely plotted as non- gausian are definitely plotted as non- gausian are definitely plotted as non- gausian uh type of probability uh type of probability uh type of probability distributions and the moment I have um distributions and the moment I have um distributions and the moment I have um and also another another sort of evident and also another another sort of evident and also another another sort of evident uh that uh thing that is actually going uh that uh thing that is actually going uh that uh thing that is actually going on with this problem is that there is a on with this problem is that there is a on with this problem is that there is a very strong overlap between the two uh very strong overlap between the two uh very strong overlap between the two uh histograms when we have the signal histograms when we have the signal histograms when we have the signal strength between no attacks and attacks strength between no attacks and attacks strength between no attacks and attacks as you can see also here from the time as you can see also here from the time as you can see also here from the time series plot uh there's a quite series plot uh there's a quite series plot uh there's a quite significant overlap between the two and significant overlap between the two and significant overlap between the two and um that's of course due to properties of um that's of course due to properties of um that's of course due to properties of radio wave propagation uh the um uh radio wave propagation uh the um uh radio wave propagation uh the um uh waveform emitted from this kind of radar waveform emitted from this kind of radar waveform emitted from this kind of radar station can be pinched on the sea station can be pinched on the sea station can be pinched on the sea surface and uh go into some kind of uh surface and uh go into some kind of uh surface and uh go into some kind of uh other transverse other kind of paths The other transverse other kind of paths The other transverse other kind of paths The so-call multipath Fading situation uh so-call multipath Fading situation uh so-call multipath Fading situation uh some of you may have observed fading some of you may have observed fading some of you may have observed fading while lening to analog radio stations uh while lening to analog radio stations uh while lening to analog radio stations uh uh such as am and FM uh back in the old uh such as am and FM uh back in the old uh such as am and FM uh back in the old days this were the only radio stations days this were the only radio stations days this were the only radio stations that we had access to uh and also you that we had access to uh and also you that we had access to uh and also you may have uh sort of experienced exactly may have uh sort of experienced exactly may have uh sort of experienced exactly the same situation uh where you are the same situation uh where you are the same situation uh where you are going in and out of coverage holes using going in and out of coverage holes using going in and out of coverage holes using your cellular devices the um important your cellular devices the um important your cellular devices the um important thing about the uh sort of thing about the uh sort of thing about the uh sort of u u u u overlap is that let's assume that I u overlap is that let's assume that I u overlap is that let's assume that I have selected here a value for my have selected here a value for my have selected here a value for my threshold threshold threshold W and uh the moment I have if you like a W and uh the moment I have if you like a W and uh the moment I have if you like a threshold W I can uh start clear uh threshold W I can uh start clear uh threshold W I can uh start clear uh defining certain areas under those defining certain areas under those defining certain areas under those probability distributions that will be probability distributions that will be probability distributions that will be of great interest to me so I want to of great interest to me so I want to of great interest to me so I want to shade the first probability uh tail shade the first probability uh tail shade the first probability uh tail which is this which is this which is this one I want also to shade this uh and area and also want to say with uh and area and also want to say with horizontal stripes this horizontal stripes this horizontal stripes this area so these three areas are I think area so these three areas are I think area so these three areas are I think will be quite important will be quite important will be quite important um now uh the overlap means that since um now uh the overlap means that since um now uh the overlap means that since there's no absolutely clear uh there's no absolutely clear uh there's no absolutely clear uh separation between the two prob separation between the two prob separation between the two prob distributions um we are always going to distributions um we are always going to distributions um we are always going to make some form of mistake we always make some form of mistake we always make some form of mistake we always going to have mistakes going to have mistakes going to have mistakes and in fact we can clearly distinguish and in fact we can clearly distinguish and in fact we can clearly distinguish uh four uh four uh four um four conditions uh and we will tabulate them conditions uh and we will tabulate them with with what we call the confusion with with what we call the confusion with with what we call the confusion Matrix uh on the one axis of the Matrix uh on the one axis of the Matrix uh on the one axis of the confusion Matrix we'll be assigning this confusion Matrix we'll be assigning this confusion Matrix we'll be assigning this axis to uh the so-call ground truth and axis to uh the so-call ground truth and axis to uh the so-call ground truth and the other to the prediction the yut the other to the prediction the yut the other to the prediction the yut and uh when the Y hat and uh when the Y hat and uh when the Y hat is positive and negative and this is positive and negative and this is positive and negative and this positive and negative agrees to the uh positive and negative agrees to the uh positive and negative agrees to the uh with the ground truth then we will uh be with the ground truth then we will uh be with the ground truth then we will uh be calling this correspondingly true calling this correspondingly true calling this correspondingly true positive and true negative uh in fact it's actually quite negative uh in fact it's actually quite common to write first the letter the common to write first the letter the common to write first the letter the second letter as a pneumonic uh rule second letter as a pneumonic uh rule second letter as a pneumonic uh rule write first the second letter and put a write first the second letter and put a write first the second letter and put a word uh the letter T in front every time word uh the letter T in front every time word uh the letter T in front every time the that you have agreement with ground the that you have agreement with ground the that you have agreement with ground truth in the uh case where you predict truth in the uh case where you predict truth in the uh case where you predict there is an attack going on but you are there is an attack going on but you are there is an attack going on but you are wrong uh you are prepending it with the wrong uh you are prepending it with the wrong uh you are prepending it with the um uh letter F stands for false so we um uh letter F stands for false so we um uh letter F stands for false so we have false positive here and here we have false positive here and here we have false positive here and here we have also false negative we are have also false negative we are have also false negative we are predicting negative but we are wrong and predicting negative but we are wrong and predicting negative but we are wrong and therefore we have the so-called false therefore we have the so-called false therefore we have the so-called false negative events the negative events the negative events the overlap as compared to the case which is overlap as compared to the case which is overlap as compared to the case which is uh uh uh quite unrealistic in practice where we quite unrealistic in practice where we quite unrealistic in practice where we have some form of significant separation have some form of significant separation have some form of significant separation between uh the uh two histograms like between uh the uh two histograms like between uh the uh two histograms like this and this and therefore it's easy to this and this and therefore it's easy to this and this and therefore it's easy to select something of a threshold W that select something of a threshold W that select something of a threshold W that will separate the two conditions will separate the two conditions will separate the two conditions perfectly the So-Cal linearly separable perfectly the So-Cal linearly separable perfectly the So-Cal linearly separable uh case um is not present here um so we uh case um is not present here um so we uh case um is not present here um so we are going to uh so let me just uh delete are going to uh so let me just uh delete are going to uh so let me just uh delete it uh to avoid any confusion is not it uh to avoid any confusion is not it uh to avoid any confusion is not present here so we always going to have present here so we always going to have present here so we always going to have in other words um these two uh type of in other words um these two uh type of in other words um these two uh type of events present uh in our uh problem and events present uh in our uh problem and events present uh in our uh problem and uh before I describe what was actually uh before I describe what was actually uh before I describe what was actually happening every time we get uh this happening every time we get uh this happening every time we get uh this threshold W to not be set optimally when threshold W to not be set optimally when threshold W to not be set optimally when the threshold W is Set uh uh too high uh the threshold W is Set uh uh too high uh the threshold W is Set uh uh too high uh then we are then we are then we are missing uh the um uh events uh that are missing uh the um uh events uh that are missing uh the um uh events uh that are actually attacks that are actually going actually attacks that are actually going actually attacks that are actually going uh will will happen and therefore we are uh will will happen and therefore we are uh will will happen and therefore we are going to be um increasing our Force going to be um increasing our Force going to be um increasing our Force negative so we are predicting that no negative so we are predicting that no negative so we are predicting that no attacks is happening while in fact they attacks is happening while in fact they attacks is happening while in fact they are and if the W uh is set too low uh we are and if the W uh is set too low uh we are and if the W uh is set too low uh we are going to be increasing the false are going to be increasing the false are going to be increasing the false positive uh rate and in fact we will be positive uh rate and in fact we will be positive uh rate and in fact we will be um alerting uh sort of um alerting uh sort of um alerting uh sort of uh the people to go down to the cube but uh the people to go down to the cube but uh the people to go down to the cube but um unfortunately no attack is actually um unfortunately no attack is actually um unfortunately no attack is actually going on now that we have recognized that we on now that we have recognized that we always going to make mistake as always going to make mistake as always going to make mistake as manifested by this manifested by this manifested by this confusion Matrix um we are uh interested to just qualify um we are uh interested to just qualify these mistakes and quantify those these mistakes and quantify those these mistakes and quantify those mistakes by just understanding uh the uh mistakes by just understanding uh the uh mistakes by just understanding uh the uh probability of uh making a probability of uh making a probability of uh making a mistake this is uh definitely the mistake this is uh definitely the mistake this is uh definitely the probability of uh when we make uh where probability of uh when we make uh where probability of uh when we make uh where our prediction why hat our prediction why hat our prediction why hat is not equal to the ground Ruth y is not equal to the ground Ruth y is not equal to the ground Ruth y and this is happens in two instances the and this is happens in two instances the and this is happens in two instances the first instance is when we first instance is when we first instance is when we have the probability when we make the uh have the probability when we make the uh have the probability when we make the uh prediction that no attack is happening prediction that no attack is happening prediction that no attack is happening when in fact there is an attack when in fact there is an attack when in fact there is an attack happening happening happening plus the plus the plus the events when we are making the opposite claim and uh I think it's worthwhile now claim and uh I think it's worthwhile now now trying to understand what is now trying to understand what is now trying to understand what is happening in this what are those happening in this what are those happening in this what are those probabilities and how they related to probabilities and how they related to probabilities and how they related to these histan we have plotted the moment these histan we have plotted the moment these histan we have plotted the moment we have we have we have um specified the threshold location over um specified the threshold location over um specified the threshold location over here w we have split the region into two here w we have split the region into two here w we have split the region into two parts the first region is called r0 and parts the first region is called r0 and parts the first region is called r0 and the second region is called R1 and um now that we have this uh uh R1 and um now that we have this uh uh region names I think the region names region names I think the region names region names I think the region names are also intuitively are also intuitively are also intuitively uh kind of understood because this zero uh kind of understood because this zero uh kind of understood because this zero index here corresponds to the case where index here corresponds to the case where index here corresponds to the case where we declare anything as we said below the we declare anything as we said below the we declare anything as we said below the threshold W is there's no attack uh uh threshold W is there's no attack uh uh threshold W is there's no attack uh uh we are predicting no attack so that's we are predicting no attack so that's we are predicting no attack so that's the what the zero is here and anything the what the zero is here and anything the what the zero is here and anything above the w we have we predicting attack above the w we have we predicting attack above the w we have we predicting attack is happening and that's why the one is is happening and that's why the one is is happening and that's why the one is there we can actually start uh putting there we can actually start uh putting there we can actually start uh putting this probabilities quantifying these this probabilities quantifying these this probabilities quantifying these probabilities based on the area uh under probabilities based on the area uh under probabilities based on the area uh under uh the those those cures uh the those those cures uh the those those cures and we I hope you all remember that and we I hope you all remember that and we I hope you all remember that probability for continuous random probability for continuous random probability for continuous random variables such a signal strength over variables such a signal strength over variables such a signal strength over here is uh sort of manifested by such here is uh sort of manifested by such here is uh sort of manifested by such kind of areas so what I'm going to do kind of areas so what I'm going to do kind of areas so what I'm going to do now is I'm going to declare that this now is I'm going to declare that this now is I'm going to declare that this probability the first probability over probability the first probability over probability the first probability over here is uh equal to the here is uh equal to the here is uh equal to the probability that I am making uh a probability that I am making uh a probability that I am making uh a prediction uh such as my prediction uh such as my prediction uh such as my uh X belongs to the region uh X belongs to the region uh X belongs to the region r0 when in fact uh the ground truth is r0 when in fact uh the ground truth is r0 when in fact uh the ground truth is uh one so I converted the Y is equal to uh one so I converted the Y is equal to uh one so I converted the Y is equal to zero to uh all the events which are uh zero to uh all the events which are uh zero to uh all the events which are uh to the left of w so all the events where to the left of w so all the events where to the left of w so all the events where X belongs to this kind of region are X belongs to this kind of region are X belongs to this kind of region are zero and uh the uh do the zero and uh the uh do the zero and uh the uh do the same belong to the region R1 same belong to the region R1 same belong to the region R1 when Y is equal to Z all events of X greater than W in Z all events of X greater than W in other words all X's which are belonging other words all X's which are belonging other words all X's which are belonging to the region R1 so it is exactly to the region R1 so it is exactly to the region R1 so it is exactly equivalent to to those uh uh convention equivalent to to those uh uh convention equivalent to to those uh uh convention that I had about what is uh y hat is that I had about what is uh y hat is that I had about what is uh y hat is equal to one and what y hat is equal to equal to one and what y hat is equal to equal to one and what y hat is equal to zero so this one uh the probability that zero so this one uh the probability that zero so this one uh the probability that X belongs to the region X belongs to the region X belongs to the region r0 uh when r0 uh when r0 uh when Y is one corresponds to the left tail of Y is one corresponds to the left tail of Y is one corresponds to the left tail of this uh probability distribution so this this uh probability distribution so this this uh probability distribution so this probability distribution but only the probability distribution but only the probability distribution but only the left tail so you can see here that the left tail so you can see here that the left tail so you can see here that the whole probability distribution here the whole probability distribution here the whole probability distribution here the whole histogram is p of X Comm y = to 1 whole histogram is p of X Comm y = to 1 whole histogram is p of X Comm y = to 1 but here I'm interesting only for the x but here I'm interesting only for the x but here I'm interesting only for the x equal to r0 so it's the summation of equal to r0 so it's the summation of equal to r0 so it's the summation of this vertically St striped and this this vertically St striped and this this vertically St striped and this bubbly kind of area over there so I am bubbly kind of area over there so I am bubbly kind of area over there so I am going to write it as an integral probability of X comma Y is = to probability of X comma Y is = to 1 DX and I'm going to take this and 1 DX and I'm going to take this and 1 DX and I'm going to take this and actually do exactly the same for the actually do exactly the same for the actually do exactly the same for the R1 probability of X comma y isal to 0 DX R1 probability of X comma y isal to 0 DX R1 probability of X comma y isal to 0 DX and now if I actually start uh relating and now if I actually start uh relating and now if I actually start uh relating what I actually wrote here with the what I actually wrote here with the what I actually wrote here with the counts that I have count it I can count counts that I have count it I can count counts that I have count it I can count through a random realization of this through a random realization of this through a random realization of this experiment um from my kind of a uh experiment um from my kind of a uh experiment um from my kind of a uh looking at the predictor output and looking at the predictor output and looking at the predictor output and looking at the ground truth over here I looking at the ground truth over here I looking at the ground truth over here I can understand that this is corresponds can understand that this is corresponds can understand that this is corresponds to the false negative to the false negative to the false negative rate and this corresponds to the false rate and this corresponds to the false rate and this corresponds to the false positive rate and uh definitely this is uh the rate and uh definitely this is uh the false negative because I am actually false negative because I am actually false negative because I am actually predicting that no attack is happening predicting that no attack is happening predicting that no attack is happening and uh uh in fact I'm wrong and the and uh uh in fact I'm wrong and the and uh uh in fact I'm wrong and the corresponding here uh case where I'm corresponding here uh case where I'm corresponding here uh case where I'm predicting that the attack is happening predicting that the attack is happening predicting that the attack is happening and in fact I'm also wrong so the false and in fact I'm also wrong so the false and in fact I'm also wrong so the false positive uh and the false neg the false positive uh and the false neg the false positive uh and the false neg the false negative and the false posi are related negative and the false posi are related negative and the false posi are related to the entries of the confusion Matrix to the entries of the confusion Matrix to the entries of the confusion Matrix here that are uh definitely present uh here that are uh definitely present uh here that are uh definitely present uh and countable using this kind of and countable using this kind of and countable using this kind of histograms and as we discussed our role histograms and as we discussed our role histograms and as we discussed our role here is to find the optimal W uh and I here is to find the optimal W uh and I here is to find the optimal W uh and I just want you to understand how visually just want you to understand how visually just want you to understand how visually we can be persuaded that is in fact we can be persuaded that is in fact we can be persuaded that is in fact there is an optimal W and that optimal W there is an optimal W and that optimal W there is an optimal W and that optimal W will minimize the probability of making will minimize the probability of making will minimize the probability of making the mistakes you cannot make make it the mistakes you cannot make make it the mistakes you cannot make make it zero because as we discussed we do not zero because as we discussed we do not zero because as we discussed we do not face in this situation uh linearly face in this situation uh linearly face in this situation uh linearly separable data set but at the very least separable data set but at the very least separable data set but at the very least we can minimize the summation of false we can minimize the summation of false we can minimize the summation of false positives and false negative positives and false negative positives and false negative events and uh imagine that that you are events and uh imagine that that you are events and uh imagine that that you are moving uh that W in the left side over moving uh that W in the left side over moving uh that W in the left side over here so trying to move this line to the here so trying to move this line to the here so trying to move this line to the left and uh look what's happening as left and uh look what's happening as left and uh look what's happening as you're moving into the left gradually you're moving into the left gradually you're moving into the left gradually you will come uh so maybe two snapshots are enough to uh so maybe two snapshots are enough to see what is see what is see what is happening so in the first uh value of happening so in the first uh value of happening so in the first uh value of this W to the left of the previous kind this W to the left of the previous kind this W to the left of the previous kind of w what we are actually uh achieving of w what we are actually uh achieving of w what we are actually uh achieving is we is we is we are uh are uh are uh going to whatever we are losing in terms going to whatever we are losing in terms going to whatever we are losing in terms of area out of our this vertically of area out of our this vertically of area out of our this vertically striped area we will be gaining in terms striped area we will be gaining in terms striped area we will be gaining in terms of the horizontally striped area having of the horizontally striped area having of the horizontally striped area having said that we actually start seeing said that we actually start seeing said that we actually start seeing reduction of this bubbly area and this reduction of this bubbly area and this reduction of this bubbly area and this area will start to be reduced and area will start to be reduced and area will start to be reduced and reduced and reduced up to the point reduced and reduced up to the point reduced and reduced up to the point where we reach what will be where we reach what will be where we reach what will be calling the W calling the W calling the W star the optimal w star the optimal w star the optimal w uh and thus optimal W is the W that uh and thus optimal W is the W that uh and thus optimal W is the W that minimized this probability of mistake minimized this probability of mistake minimized this probability of mistake simply because in that location the simply because in that location the simply because in that location the bubbly area got eliminated bubbly area got eliminated bubbly area got eliminated completely and the summation of the completely and the summation of the completely and the summation of the therefore of the false positives and therefore of the false positives and therefore of the false positives and false negatives that included it uh uh false negatives that included it uh uh false negatives that included it uh uh is the minimum is the minimum is the minimum possible so actually we can write that possible so actually we can write that possible so actually we can write that uh sort of optimizing the W towards W the W towards W star uh will be done done using the algorithm that algorithm that uh minimizes the minimizes the probability of mistakes probability of making a mistake mistakes probability of making a mistake so the misclassification error or also so the misclassification error or also so the misclassification error or also called misclassification error eight so this misclassification error eight so this will be done now that we have some kind will be done now that we have some kind will be done now that we have some kind of a visual motivation of what we're of a visual motivation of what we're of a visual motivation of what we're trying to achieve here we now need to trying to achieve here we now need to trying to achieve here we now need to understand how we can also motivate um understand how we can also motivate um understand how we can also motivate um in the next discussion an objective in the next discussion an objective in the next discussion an objective function and that it is going to be function and that it is going to be function and that it is going to be suitable for uh our uh problem here suitable for uh our uh problem here suitable for uh our uh problem here which is the classification problem in a which is the classification problem in a which is the classification problem in a similar way as we have done with the U similar way as we have done with the U similar way as we have done with the U earlier loss function we have used earlier loss function we have used earlier loss function we have used initially was called mean square error initially was called mean square error initially was called mean square error and then it was also called cross and then it was also called cross and then it was also called cross entropy so we'll do that entropy so we'll do that entropy so we'll do that next the come up with uh this objective next the come up with uh this objective next the come up with uh this objective function before we go and discuss that function before we go and discuss that function before we go and discuss that let's let's let's review uh the so-called classification review uh the so-called classification review uh the so-called classification metrics the classification metrics that metrics the classification metrics that metrics the classification metrics that uh there a couple of classification uh there a couple of classification uh there a couple of classification metrics will be of interest to us they metrics will be of interest to us they metrics will be of interest to us they will be entirely based of course on the will be entirely based of course on the will be entirely based of course on the previously described confusion Matrix previously described confusion Matrix previously described confusion Matrix and the first matric I want to address and the first matric I want to address and the first matric I want to address is called the true positive is called the true positive is called the true positive rate the second metric is well the the rate the second metric is well the the rate the second metric is well the the Dr postive rate is comes with many names Dr postive rate is comes with many names Dr postive rate is comes with many names um and many of them have been sort of um and many of them have been sort of um and many of them have been sort of originating from various kind of domains originating from various kind of domains originating from various kind of domains electrical engineering computer electrical engineering computer electrical engineering computer science um and others so uh in computer science um and others so uh in computer science um and others so uh in computer science this also is called science this also is called science this also is called recall in electrical engineering this is recall in electrical engineering this is recall in electrical engineering this is also called probability of detection and also called probability of detection and also called probability of detection and many other domains quote it as many other domains quote it as many other domains quote it as sensitivity sensitivity sensitivity it's one and the same thing and I just it's one and the same thing and I just it's one and the same thing and I just want to mention all of them just in case want to mention all of them just in case want to mention all of them just in case you come up with um come across one of you come up with um come across one of you come up with um come across one of the of the many so this is the ratio the of the many so this is the ratio the of the many so this is the ratio between true between true between true positive and true positive plus false positive and true positive plus false positive and true positive plus false negative so this is a a ratio that uh is negative so this is a a ratio that uh is negative so this is a a ratio that uh is definitely going to uh be of of concern definitely going to uh be of of concern definitely going to uh be of of concern to us and and of interest to us every to us and and of interest to us every to us and and of interest to us every time we have to evaluate a classifier time we have to evaluate a classifier time we have to evaluate a classifier and the second metrix that I want to and the second metrix that I want to and the second metrix that I want to quote and have some discussion about quote and have some discussion about quote and have some discussion about those metrics a bit later is a so-called those metrics a bit later is a so-called those metrics a bit later is a so-called precision and this Precision is another precision and this Precision is another precision and this Precision is another ratio of true positive IDE by true ratio of true positive IDE by true ratio of true positive IDE by true positive plus false positive and if you positive plus false positive and if you positive plus false positive and if you follow the this video uh where we have follow the this video uh where we have follow the this video uh where we have plotted these histograms in the binary plotted these histograms in the binary plotted these histograms in the binary classifier when the so-called the radar classifier when the so-called the radar classifier when the so-called the radar problem and you probably uh understood problem and you probably uh understood problem and you probably uh understood the tradeoff that exists between false the tradeoff that exists between false the tradeoff that exists between false positives and false negatives as we were positives and false negatives as we were positives and false negatives as we were moving in fact uh the uh as we were moving in fact uh the uh as we were moving in fact uh the uh as we were moving the value of the threshold uh w moving the value of the threshold uh w moving the value of the threshold uh w we were changing uh the areas under we were changing uh the areas under we were changing uh the areas under those two histograms and of course uh those two histograms and of course uh those two histograms and of course uh here we we were trading all false here we we were trading all false here we we were trading all false positive or false negatives in our positive or false negatives in our positive or false negatives in our attempt to find this optimal kind of W attempt to find this optimal kind of W attempt to find this optimal kind of W uh in a very similar way we can actually uh in a very similar way we can actually uh in a very similar way we can actually claim that now that we have uh the those claim that now that we have uh the those claim that now that we have uh the those metrics uh metrics uh metrics uh the tradeoff between false positives and the tradeoff between false positives and the tradeoff between false positives and false negatives is evident over here U false negatives is evident over here U false negatives is evident over here U uh in in in the following trade off so uh in in in the following trade off so uh in in in the following trade off so let me write it this let me write it this let me write it this down uh so we can say that down uh so we can say that down uh so we can say that because as W uh false positives and false uh false positives and false negatives uh we can actually claim that negatives uh we can actually claim that negatives uh we can actually claim that there is a between uh between uh recall and recall and recall and precision because recall and precision precision because recall and precision precision because recall and precision everything is exactly the same in terms everything is exactly the same in terms everything is exactly the same in terms of numerator and portion of the of numerator and portion of the of numerator and portion of the denominator but only the uh false POS denominator but only the uh false POS denominator but only the uh false POS and false negatives are present there so and false negatives are present there so and false negatives are present there so this is actually an important tradeoff this is actually an important tradeoff this is actually an important tradeoff that will be of of great interest to us that will be of of great interest to us that will be of of great interest to us uh as we will always finding ourselves uh as we will always finding ourselves uh as we will always finding ourselves uh making that kind of tradeoff for uh making that kind of tradeoff for uh making that kind of tradeoff for classif classification architectures we classif classification architectures we classif classification architectures we will be designing soon uh the other will be designing soon uh the other will be designing soon uh the other um metric it's not really a different um metric it's not really a different um metric it's not really a different metric but it's a way to present uh metric but it's a way to present uh metric but it's a way to present uh performance metrics classification performance metrics classification performance metrics classification metrix is this uh what we call the metrix is this uh what we call the metrix is this uh what we call the So-Cal receiver operating characteristic So-Cal receiver operating characteristic So-Cal receiver operating characteristic and we actually call it receiver and we actually call it receiver and we actually call it receiver operating characteristic from uh those operating characteristic from uh those operating characteristic from uh those days in the 40s when they were deploying days in the 40s when they were deploying days in the 40s when they were deploying this kind of this kind of this kind of Radars and I will describe it as uh the Radars and I will describe it as uh the Radars and I will describe it as uh the curve uh that we can plot by changing curve uh that we can plot by changing curve uh that we can plot by changing the threshold W in the x-axis over here the threshold W in the x-axis over here the threshold W in the x-axis over here it is the false positive it is the false positive it is the false positive rate also known as a false alarm from rate also known as a false alarm from rate also known as a false alarm from those days the probab ility of uh the those days the probab ility of uh the those days the probab ility of uh the probability of false alarm PFA and the Y probability of false alarm PFA and the Y probability of false alarm PFA and the Y AIS is called recall evidently the same AIS is called recall evidently the same AIS is called recall evidently the same thing as a true positive rate and thing as a true positive rate and thing as a true positive rate and definitely we have a probability of definitely we have a probability of definitely we have a probability of false positive rate that goes from one false positive rate that goes from one false positive rate that goes from one uh from 0 to one and the probability of uh from 0 to one and the probability of uh from 0 to one and the probability of recall true positive rate that goes recall true positive rate that goes recall true positive rate that goes again from 0 to one because there are again from 0 to one because there are again from 0 to one because there are probabilities and therefore we'll find probabilities and therefore we'll find probabilities and therefore we'll find this Cur of constraint by those values and the and the U as we change the U as we change the U as we change the threshold uh we will be able to plot to draw such draw such cures some of these curves are going to cures some of these curves are going to cures some of these curves are going to be like this let me plot three cases so case let's say cases so case let's say a case b and case C a case b and case C a case b and case C and I think it's reasonable to and I think it's reasonable to and I think it's reasonable to understand now what is uh the best understand now what is uh the best understand now what is uh the best possible classifier we can ever design possible classifier we can ever design possible classifier we can ever design which is of course not achievable uh which is of course not achievable uh which is of course not achievable uh right now um and it's not achievable in right now um and it's not achievable in right now um and it's not achievable in any case we have uh such cases of any case we have uh such cases of any case we have uh such cases of overlap between positive and negative overlap between positive and negative overlap between positive and negative classes and that uh point over here is classes and that uh point over here is classes and that uh point over here is uh this is the sort of go of uh this is the sort of go of uh this is the sort of go of Ideal and unrealistic operating point this care as we discussed is called this care as we discussed is called receiver R and every and each and every curve is R and every and each and every curve is being plotted by having a being plotted by having a being plotted by having a classifier and tuning if you like it's classifier and tuning if you like it's classifier and tuning if you like it's par par par ameters adjusting its parameters the ameters adjusting its parameters the ameters adjusting its parameters the threshold more specifically and same threshold more specifically and same threshold more specifically and same here this is supposed to be a diagonal here this is supposed to be a diagonal here this is supposed to be a diagonal line line line 45° um and let's compare between uh 45° um and let's compare between uh 45° um and let's compare between uh three different three different three different classifiers um which one uh do we classifiers um which one uh do we classifiers um which one uh do we believe that is actually the best one um believe that is actually the best one um believe that is actually the best one um and it's actually a very uh and it's actually a very uh and it's actually a very uh straightforward kind of answer uh if we straightforward kind of answer uh if we straightforward kind of answer uh if we draw let's say um a horizontal kind of L draw let's say um a horizontal kind of L draw let's say um a horizontal kind of L line uh the classifier a is offering line uh the classifier a is offering line uh the classifier a is offering exactly uh the uh same performance as exactly uh the uh same performance as exactly uh the uh same performance as the the the recall but at a much reduced probability recall but at a much reduced probability recall but at a much reduced probability of false alarm or false positive rate as of false alarm or false positive rate as of false alarm or false positive rate as compared to classifier compared to classifier compared to classifier B and of course much much better than B and of course much much better than B and of course much much better than classifier C and therefore either we classifier C and therefore either we classifier C and therefore either we draw a horizontal line or actually you draw a horizontal line or actually you draw a horizontal line or actually you can actually draw a vertical line we can can actually draw a vertical line we can can actually draw a vertical line we can make the same argument a is offered a make the same argument a is offered a make the same argument a is offered a much better uh probability of uh uh true much better uh probability of uh uh true much better uh probability of uh uh true positive uh as compared to uh B and as positive uh as compared to uh B and as positive uh as compared to uh B and as compared to C for the same uh false compared to C for the same uh false compared to C for the same uh false positive rate and therefore we can positive rate and therefore we can positive rate and therefore we can actually write this kind of preference actually write this kind of preference actually write this kind of preference relationship in this specific case so relationship in this specific case so relationship in this specific case so recall the um recall and force positive recall the um recall and force positive recall the um recall and force positive are involved in plotting this kind of are involved in plotting this kind of are involved in plotting this kind of curve to give us a if you like a curve to give us a if you like a curve to give us a if you like a graphical view of how the classifier is graphical view of how the classifier is graphical view of how the classifier is is behaving in various kind of operating is behaving in various kind of operating is behaving in various kind of operating conditions and when we tune this conditions and when we tune this conditions and when we tune this classifier and we choosing the W the classifier and we choosing the W the classifier and we choosing the W the specific W start that we have seen specific W start that we have seen specific W start that we have seen earlier we effectively operate at a earlier we effectively operate at a earlier we effectively operate at a specific operating point at that point specific operating point at that point specific operating point at that point we will be um sort of constantly we will be um sort of constantly we will be um sort of constantly operating and uh we will uh in many operating and uh we will uh in many operating and uh we will uh in many senses we will need to make different senses we will need to make different senses we will need to make different tradeoffs uh between positives and uh tradeoffs uh between positives and uh tradeoffs uh between positives and uh false positives and true positives in false positives and true positives in false positives and true positives in various various various applications so let me write this down applications so let me write this down applications so let me write this down that uh uh each point in the point in the ROC unique unique setting setting setting of the threshold W so that is uh you know as kind of a W so that is uh you know as kind of a short summary of um a couple of short summary of um a couple of short summary of um a couple of classification metrics that will be of classification metrics that will be of classification metrics that will be of interest to us and uh the receiver interest to us and uh the receiver interest to us and uh the receiver operating curve uh and we will also have operating curve uh and we will also have operating curve uh and we will also have another curve called recall versus another curve called recall versus another curve called recall versus Precision this Curve will be introduced Precision this Curve will be introduced Precision this Curve will be introduced in another video um and we'll be in another video um and we'll be in another video um and we'll be discussed uh uh then discussed uh uh then discussed uh uh then in an early video we saw how maximum in an early video we saw how maximum in an early video we saw how maximum likelihood was motivating every uh uh likelihood was motivating every uh uh likelihood was motivating every uh uh the underlying you like objective the underlying you like objective the underlying you like objective function of of every prediction problem function of of every prediction problem function of of every prediction problem and so binary classification will not be and so binary classification will not be and so binary classification will not be an an an exception uh and we started with the exception uh and we started with the exception uh and we started with the regression problem and we saw how regression problem and we saw how regression problem and we saw how maximum likelihood and cross entropy are maximum likelihood and cross entropy are maximum likelihood and cross entropy are ultimately connected now we recognize uh ultimately connected now we recognize uh ultimately connected now we recognize uh the functional form of the so-call the functional form of the so-call the functional form of the so-call binary crossentropy loss function which binary crossentropy loss function which binary crossentropy loss function which is uh for spe specifically for our is uh for spe specifically for our is uh for spe specifically for our binary classification problem and we binary classification problem and we binary classification problem and we will be motivating this by recognizing will be motivating this by recognizing will be motivating this by recognizing that uh in regression we had a p mod that uh in regression we had a p mod that uh in regression we had a p mod which actually was gausian in binary which actually was gausian in binary which actually was gausian in binary classification we going to need to classification we going to need to classification we going to need to actually uh have a probabilistic model a actually uh have a probabilistic model a actually uh have a probabilistic model a probability distribution which is really probability distribution which is really probability distribution which is really appropriate for our uh discrete uh appropriate for our uh discrete uh appropriate for our uh discrete uh random variables uh that are the are random variables uh that are the are random variables uh that are the are wise so our form of myp model of let's wise so our form of myp model of let's wise so our form of myp model of let's say y given an X comma discrete and in fact binary so I have discrete and in fact binary so I have seen already um in the discussion of the seen already um in the discussion of the seen already um in the discussion of the entropy video uh coing and I know that entropy video uh coing and I know that entropy video uh coing and I know that at that point I have quoted beri at that point I have quoted beri at that point I have quoted beri distribution as distribution as distribution as uh uh the appropriate propability uh uh the appropriate propability uh uh the appropriate propability distribution for our uh model and the distribution for our uh model and the distribution for our uh model and the beri distribution let me write it with beri distribution let me write it with beri distribution let me write it with all words over here is given as y hat to the power of Y here is given as y hat to the power of Y 1 - 1 - 1 - y the^ of 1 - y let's spend some time y the^ of 1 - y let's spend some time y the^ of 1 - y let's spend some time kind of understanding this uh if kind of understanding this uh if kind of understanding this uh if my ground truth is my ground truth is my ground truth is one uh this P model of Y model of Y had of Y sorry given had of Y sorry given had of Y sorry given X comma W is simply X comma W is simply X comma W is simply y uh because uh the Y is is one so this y uh because uh the Y is is one so this y uh because uh the Y is is one so this is zero and therefore this term hole is is zero and therefore this term hole is is zero and therefore this term hole is one so the only thing that remains is y one so the only thing that remains is y one so the only thing that remains is y and in fact this is a very important uh and in fact this is a very important uh and in fact this is a very important uh conclusion conclusion conclusion uh in a sense that uh uh from now uh in a sense that uh uh from now uh in a sense that uh uh from now on uh in binary classification all I on uh in binary classification all I on uh in binary classification all I need to produce at the output is uh this need to produce at the output is uh this need to produce at the output is uh this uh uh a single floating Point number uh uh a single floating Point number uh uh a single floating Point number between zero and one so it's a between zero and one so it's a between zero and one so it's a probability so it's going to be zero probability so it's going to be zero probability so it's going to be zero between Z and one for sure I'll be between Z and one for sure I'll be between Z and one for sure I'll be calling this probability uh when Y is calling this probability uh when Y is calling this probability uh when Y is equal to 1 the probability of Y is equal equal to 1 the probability of Y is equal equal to 1 the probability of Y is equal to 1 given X comma w in fact uh I can to 1 given X comma w in fact uh I can to 1 given X comma w in fact uh I can write it as write it as write it as a the model and this is actually going to be model and this is actually going to be called the posterior uh probability and we posterior uh probability and we recognize the term posterior already we recognize the term posterior already we recognize the term posterior already we recognized it when we had this recognized it when we had this recognized it when we had this probability review uh lecture uh and in probability review uh lecture uh and in probability review uh lecture uh and in that kind of video we have actually seen that kind of video we have actually seen that kind of video we have actually seen that uh posterior it means after we get that uh posterior it means after we get that uh posterior it means after we get to observe the AIS our uh data what can to observe the AIS our uh data what can to observe the AIS our uh data what can we say about our uh Target variable Y in we say about our uh Target variable Y in we say about our uh Target variable Y in this case so the posterior probability this case so the posterior probability this case so the posterior probability distribution is going to be called yck distribution is going to be called yck distribution is going to be called yck so that's uh that is what our classifier so that's uh that is what our classifier so that's uh that is what our classifier is going to from now on uh going to be is going to from now on uh going to be is going to from now on uh going to be producing and that this posterior being producing and that this posterior being producing and that this posterior being a probability means that we are uh have a probability means that we are uh have a probability means that we are uh have inherent the ability to uh provide an inherent the ability to uh provide an inherent the ability to uh provide an uncertainty about our pred uncertainty about our pred uncertainty about our pred prediction we are going to uh let's say prediction we are going to uh let's say prediction we are going to uh let's say report the report the report the 0.82 as let's say the output of the 0.82 as let's say the output of the 0.82 as let's say the output of the positive class and this means that we positive class and this means that we positive class and this means that we are going to be 82% certain that we have are going to be 82% certain that we have are going to be 82% certain that we have a positive uh event at the output of our a positive uh event at the output of our a positive uh event at the output of our classif fire and if we have already classif fire and if we have already classif fire and if we have already reported the output of a positive event reported the output of a positive event reported the output of a positive event then uh when we are dealing with the then uh when we are dealing with the then uh when we are dealing with the negative case our P model over here that negative case our P model over here that negative case our P model over here that we have selected of Y given X comma W we have selected of Y given X comma W we have selected of Y given X comma W will be simply be 1 minus y will be simply be 1 minus y will be simply be 1 minus y because uh with Y is equal to zero then because uh with Y is equal to zero then because uh with Y is equal to zero then this becomes one and this becomes one this becomes one and this becomes one this becomes one and this becomes one minus y hat so immediately we can minus y hat so immediately we can minus y hat so immediately we can actually actually actually get the corresponding uh uh P model for get the corresponding uh uh P model for get the corresponding uh uh P model for uh the C of the negative case in fact I uh the C of the negative case in fact I uh the C of the negative case in fact I don't need to write anything else in don't need to write anything else in don't need to write anything else in this this this point maybe I can do maybe I can write P point maybe I can do maybe I can write P point maybe I can do maybe I can write P model of Y is equal to Z given X comma W model of Y is equal to Z given X comma W model of Y is equal to Z given X comma W and this was shown to be 1 - y so I and this was shown to be 1 - y so I and this was shown to be 1 - y so I don't even need to produce uh a vector don't even need to produce uh a vector don't even need to produce uh a vector in the output of my binary classifier in the output of my binary classifier in the output of my binary classifier just with one value uh from that value I just with one value uh from that value I just with one value uh from that value I can deterministically obtain the other can deterministically obtain the other can deterministically obtain the other for the negative class so if I have for the negative class so if I have for the negative class so if I have produced 0.8 for the positive class then produced 0.8 for the positive class then produced 0.8 for the positive class then the probability uh of the negative class the probability uh of the negative class the probability uh of the negative class is 0.2 is 0.2 is 0.2 okay so that's uh kind of an important okay so that's uh kind of an important okay so that's uh kind of an important conclusion of this kind of discussion conclusion of this kind of discussion conclusion of this kind of discussion and the uh beri distribution which is and the uh beri distribution which is and the uh beri distribution which is obviously very appropriate for uh binary obviously very appropriate for uh binary obviously very appropriate for uh binary events at the 0o one or the So-Cal coin events at the 0o one or the So-Cal coin events at the 0o one or the So-Cal coin dossing distribution of heads uh versus dossing distribution of heads uh versus dossing distribution of heads uh versus tales and if I remember uh the um uh the tales and if I remember uh the um uh the tales and if I remember uh the um uh the maximum lack kind of discussion and the maximum lack kind of discussion and the maximum lack kind of discussion and the so-called cross entropy in that kind of so-called cross entropy in that kind of so-called cross entropy in that kind of cross entropy I had the following uh cross entropy I had the following uh cross entropy I had the following uh formula which is equally applicable to formula which is equally applicable to formula which is equally applicable to any predictor I had a y cat comma Y is any predictor I had a y cat comma Y is any predictor I had a y cat comma Y is minus the minus the minus the expectation of X comma expectation of X comma expectation of X comma y my examples according to the so-call P y my examples according to the so-call P y my examples according to the so-call P data High data High data High distribution uh log P model of Y given X comma model of Y given X comma W and this W and this W and this is uh definitely um the the result that is uh definitely um the the result that is uh definitely um the the result that we have seen uh earlier and just want to we have seen uh earlier and just want to we have seen uh earlier and just want to contrast that against uh you know what contrast that against uh you know what contrast that against uh you know what we have seen earlier we had an we have seen earlier we had an we have seen earlier we had an underlying probability distribution underlying probability distribution underlying probability distribution where our data is and uh uh definitely where our data is and uh uh definitely where our data is and uh uh definitely this P data had distribution is uh this P data had distribution is uh this P data had distribution is uh effectively the uh table that we have effectively the uh table that we have effectively the uh table that we have seen seen seen earlier the distribution that governs earlier the distribution that governs earlier the distribution that governs the uh training data uh or the data that the uh training data uh or the data that the uh training data uh or the data that we we have recorded uh with all our kind we we have recorded uh with all our kind we we have recorded uh with all our kind of ground truths so the BET data High of ground truths so the BET data High of ground truths so the BET data High distribution is present in here uh so no distribution is present in here uh so no distribution is present in here uh so no no changes what we have seen earlier no changes what we have seen earlier no changes what we have seen earlier it's just a different type of data and it's just a different type of data and it's just a different type of data and I'm actually interested to um go ahead I'm actually interested to um go ahead I'm actually interested to um go ahead and calculate this term over here and and calculate this term over here and and calculate this term over here and this term can be trivially calculated as this term can be trivially calculated as this term can be trivially calculated as log of uh my P model is uh why to^ of y log of uh my P model is uh why to^ of y log of uh my P model is uh why to^ of y * 1 - y the 1 - y or * 1 - y the 1 - y or * 1 - y the 1 - y or log y to the^ of Y plus log of 1 - y to log y to the^ of Y plus log of 1 - y to log y to the^ of Y plus log of 1 - y to the^ of 1 - Y and this is uh U of course the^ of 1 - Y and this is uh U of course the^ of 1 - Y and this is uh U of course making the use of the uh log of the making the use of the uh log of the making the use of the uh log of the product is the summation of the logs product is the summation of the logs product is the summation of the logs identity and this is uses another identity and this is uses another identity and this is uses another identity this is y log of y hat + 1 - y identity this is y log of y hat + 1 - y identity this is y log of y hat + 1 - y log of 1 - y hat and for those who log of 1 - y hat and for those who log of 1 - y hat and for those who remember the discussion of the entropy remember the discussion of the entropy remember the discussion of the entropy we have seen a sort of identical kind of we have seen a sort of identical kind of we have seen a sort of identical kind of term in the uh presentation of the term in the uh presentation of the term in the uh presentation of the binary kind of entropy uh uh graph the binary kind of entropy uh uh graph the binary kind of entropy uh uh graph the coin tossing during the the coin tossing coin tossing during the the coin tossing coin tossing during the the coin tossing experiment so let's uh plug this in into experiment so let's uh plug this in into experiment so let's uh plug this in into this formula and at the same time we this formula and at the same time we this formula and at the same time we will replace the expectation with a will replace the expectation with a will replace the expectation with a sample mean so from these two we can sample mean so from these two we can sample mean so from these two we can conclude that the cross entropy the conclude that the cross entropy the conclude that the cross entropy the binary cross entropy so I'm just going binary cross entropy so I'm just going binary cross entropy so I'm just going to put a capital B in front to to put a capital B in front to to put a capital B in front to distinguish for the cross entropy for distinguish for the cross entropy for distinguish for the cross entropy for multiclass problems it is y hat comma multiclass problems it is y hat comma multiclass problems it is y hat comma y minus 1 / M summation from I is equal y minus 1 / M summation from I is equal y minus 1 / M summation from I is equal to 1 to m to 1 to m to 1 to m of Yi log of Yi Yi log of Yi hat plus 1 - y hat definitely this is uh a very easy to hat definitely this is uh a very easy to evaluate this uh is definitely a a evaluate this uh is definitely a a evaluate this uh is definitely a a scaler which is indicates to me whether scaler which is indicates to me whether scaler which is indicates to me whether how well am I doing how well am I doing how well am I doing and uh this is the loss function that is and uh this is the loss function that is and uh this is the loss function that is going to govern from now on uh our going to govern from now on uh our going to govern from now on uh our binary classification uh problem and uh binary classification uh problem and uh binary classification uh problem and uh we are need to use it as an objective we are need to use it as an objective we are need to use it as an objective function and when we minimize it function and when we minimize it function and when we minimize it equivalently we will be reducing the equivalently we will be reducing the equivalently we will be reducing the probabilistic distance between the P probabilistic distance between the P probabilistic distance between the P data hack uh that uh is present in my uh data hack uh that uh is present in my uh data hack uh that uh is present in my uh data set that is actually given to me data set that is actually given to me data set that is actually given to me and the P model which as we discussed and the P model which as we discussed and the P model which as we discussed here is of the beri distribution here is of the beri distribution here is of the beri distribution and now we can just go ahead and plot and now we can just go ahead and plot and now we can just go ahead and plot that in fact let me just go and plot that in fact let me just go and plot that in fact let me just go and plot this term and this term can term and this term can be plotted over here so the x-axis I'm going to plot it here so the x-axis I'm going to plot it against the Y hat the Y is for uh the against the Y hat the Y is for uh the against the Y hat the Y is for uh the probability of the probability of the probability of the positive uh and uh this is my binary positive uh and uh this is my binary positive uh and uh this is my binary cross entropy in fact I will call this cross entropy in fact I will call this cross entropy in fact I will call this term the so-called inner term to avoid term the so-called inner term to avoid term the so-called inner term to avoid confusion with something that involves confusion with something that involves confusion with something that involves an averaging over many of those inner an averaging over many of those inner an averaging over many of those inner terms for each of the terms for each of the terms for each of the examples uh and each of my predictions examples uh and each of my predictions examples uh and each of my predictions uh I have obviously um a term over here uh I have obviously um a term over here uh I have obviously um a term over here a real number I can trivially calculate a real number I can trivially calculate a real number I can trivially calculate so let me call it the inner term over so let me call it the inner term over so let me call it the inner term over here in the Y AIS and if I plot this here in the Y AIS and if I plot this here in the Y AIS and if I plot this term then I will get something that is term then I will get something that is term then I will get something that is going to look like this so the uh probability let's this so the uh probability let's interpret this um uh graph uh when the y interpret this um uh graph uh when the y interpret this um uh graph uh when the y height is one means that I am 100% height is one means that I am 100% height is one means that I am 100% certain that uh this is a positive event certain that uh this is a positive event certain that uh this is a positive event that has happened and this means that that has happened and this means that that has happened and this means that the uh uh probability so this curve the uh uh probability so this curve the uh uh probability so this curve corresponds with uh I forgot to mention corresponds with uh I forgot to mention corresponds with uh I forgot to mention that this curve corresponds is plotted that this curve corresponds is plotted that this curve corresponds is plotted when Y is equal to 1 for the specific when Y is equal to 1 for the specific when Y is equal to 1 for the specific ground truth when my ground truth agrees ground truth when my ground truth agrees ground truth when my ground truth agrees with me then I'm expected to get um uh with me then I'm expected to get um uh with me then I'm expected to get um uh the inner term to be zero as intuitively the inner term to be zero as intuitively the inner term to be zero as intuitively understood I'm 100% in agreement with understood I'm 100% in agreement with understood I'm 100% in agreement with the uh ground truth here uh the um on the uh ground truth here uh the um on the uh ground truth here uh the um on the other hand when I'm predicting the Y the other hand when I'm predicting the Y the other hand when I'm predicting the Y had to be uh 0 point let's say 05 over had to be uh 0 point let's say 05 over had to be uh 0 point let's say 05 over here and this means that I'm here and this means that I'm here and this means that I'm predicting uh the positive to be predicting uh the positive to be predicting uh the positive to be 0.05 and uh in other words I'm 0.05 and uh in other words I'm 0.05 and uh in other words I'm predicting the negative to be predicting the negative to be predicting the negative to be 0.95 I'm predicting that a no attack has 0.95 I'm predicting that a no attack has 0.95 I'm predicting that a no attack has happened with 95% happened with 95% happened with 95% confidence um then I'm actually going to confidence um then I'm actually going to confidence um then I'm actually going to and my ground Ruth disagrees with me I and my ground Ruth disagrees with me I and my ground Ruth disagrees with me I am going to incure a huge am going to incure a huge am going to incure a huge loss uh so I can actually sufficient to loss uh so I can actually sufficient to loss uh so I can actually sufficient to say here that BC say here that BC say here that BC uh decisions such as decisions such as 0.95 but wrong decisions and actually can draw also the decisions and actually can draw also the uh kind of corresponding care zero and it should be symmetric zero and it should be symmetric obviously in the handwritten kind of way obviously in the handwritten kind of way obviously in the handwritten kind of way this is they are not symmetric here but this is they are not symmetric here but this is they are not symmetric here but this is kind of quite important to this is kind of quite important to this is kind of quite important to realize the behavior of binary cross realize the behavior of binary cross realize the behavior of binary cross penalizing confident wrong decisions so now we have uh everything that we so now we have uh everything that we need to draw a block diagram and uh this need to draw a block diagram and uh this need to draw a block diagram and uh this block diagram is applicable in fact block diagram is applicable in fact block diagram is applicable in fact to all sorts of predictors and we have to all sorts of predictors and we have to all sorts of predictors and we have already um seen regression now in already um seen regression now in already um seen regression now in classification so let me just throw the classification so let me just throw the classification so let me just throw the predictor as this box over here we have seen that in the regression here we have seen that in the regression setting a linear model a DOT product in setting a linear model a DOT product in setting a linear model a DOT product in other words between uh the parameter other words between uh the parameter other words between uh the parameter vector and features and we also seeing vector and features and we also seeing vector and features and we also seeing here um a classifier but we have not here um a classifier but we have not here um a classifier but we have not really discussed yet the functional form really discussed yet the functional form really discussed yet the functional form of what the classifier will actually be of what the classifier will actually be of what the classifier will actually be but whatever this classifier is uh is but whatever this classifier is uh is but whatever this classifier is uh is going to uh so this is either for going to uh so this is either for going to uh so this is either for aggression or classification uh and input X is going classification uh and input X is going to come to come to come in in general it has many dimensions a y in in general it has many dimensions a y in in general it has many dimensions a y height is going to be produced at the height is going to be produced at the height is going to be produced at the output and uh over here we're going to output and uh over here we're going to output and uh over here we're going to have the um let's assume that this is have the um let's assume that this is have the um let's assume that this is now for now for now for U I mean let's call it a loss function U I mean let's call it a loss function U I mean let's call it a loss function and the loss function will be the mean and the loss function will be the mean and the loss function will be the mean square error um or uh the the cross square error um or uh the the cross square error um or uh the the cross entropy in fact the we will show that entropy in fact the we will show that entropy in fact the we will show that the cross entropy um is able to the cross entropy um is able to the cross entropy um is able to accommodate both mean square error um as accommodate both mean square error um as accommodate both mean square error um as well uh and as as well also the binary well uh and as as well also the binary well uh and as as well also the binary concenter we we have just seen so the concenter we we have just seen so the concenter we we have just seen so the output of this kind of loss function so output of this kind of loss function so output of this kind of loss function so this loss function should have some this loss function should have some this loss function should have some knowledge of the ground ruths Y and uh knowledge of the ground ruths Y and uh knowledge of the ground ruths Y and uh we are going to obtain a scalar number we are going to obtain a scalar number we are going to obtain a scalar number uh of the loss uh for um and then the scalar loss uh for um and then the scalar number number number is going to be effectively fed into a is going to be effectively fed into a is going to be effectively fed into a block that it will calculate the values block that it will calculate the values block that it will calculate the values of the gradient of the loss with respect of the gradient of the loss with respect of the gradient of the loss with respect to the parameter Vector W and this is part of the stochastic gr W and this is part of the stochastic gr descent algorithm we have seen and the descent algorithm we have seen and the descent algorithm we have seen and the output of this block is going to be output of this block is going to be output of this block is going to be called the param parameter update that we will accept a learning rate that that we will accept a learning rate that we actually call we actually call we actually call EA this uh will also accept as a high EA this uh will also accept as a high EA this uh will also accept as a high parameter the mini batch that we have parameter the mini batch that we have parameter the mini batch that we have actually called actually called actually called MB and uh it will the parameter update MB and uh it will the parameter update MB and uh it will the parameter update formula will provide for us and will formula will provide for us and will formula will provide for us and will update for us the vector W of all the update for us the vector W of all the update for us the vector W of all the parameters involved inside this parameters involved inside this parameters involved inside this predictor so this is a very generic predictor so this is a very generic predictor so this is a very generic block diagram that we allow us to uh block diagram that we allow us to uh block diagram that we allow us to uh train and uh therefore optimize any any uh machine Any optimize any any uh machine Any prediction machine we have uh seen up to prediction machine we have uh seen up to prediction machine we have uh seen up to this point\" (Video eQ6UE968Xe4, 1.9-3103.5s)\n"
          ]
        }
      ],
      "source": [
        "def answer_query(query, top_k=1):\n",
        "    q_vec = model.encode([query], convert_to_numpy=True)[0]\n",
        "    results = client.search(collection_name=collection_name, query_vector=q_vec, limit=top_k)\n",
        "    top = results[0]  # best match\n",
        "    data = top.payload\n",
        "    return data[\"video_id\"], data[\"start\"], data[\"end\"], data[\"sentence\"]\n",
        "\n",
        "# Example query\n",
        "video_id, start, end, sentence = answer_query(\"Explain how convolutional layers work\")\n",
        "print(f\"Top answer: \\\"{sentence}\\\" (Video {video_id}, {start:.1f}-{end:.1f}s)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzPmr_EHTGnr",
        "outputId": "2dbd6e6b-2db9-42ea-f5c1-25d6d1027db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved clip to clip_eQ6UE968Xe4_1_3103.mp4\n"
          ]
        }
      ],
      "source": [
        "import ffmpeg\n",
        "\n",
        "def extract_clip(video_id, start, end, output_path=\"clip.mp4\"):\n",
        "    # Find the video file for this ID\n",
        "    mp4_files = glob.glob(os.path.join(\"./data/videos\", video_id, \"*.mp4\"))\n",
        "    if not mp4_files:\n",
        "        raise FileNotFoundError(f\"No video found for ID {video_id}\")\n",
        "    video_path = mp4_files[0]\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(video_path, ss=start, to=end)\n",
        "        .output(output_path, codec=\"copy\")\n",
        "        .run(overwrite_output=True)\n",
        "    )\n",
        "    return output_path\n",
        "\n",
        "# Example: extract the clip for the top result\n",
        "clip_file = extract_clip(video_id, start, end, output_path=f\"clip_{video_id}_{int(start)}_{int(end)}.mp4\")\n",
        "print(f\"Saved clip to {clip_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc08UkYATGns",
        "outputId": "bb1df953-c655-4f48-d0a8-c00c1a5d267c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gradio extension is already loaded. To reload it, use:\n",
            "  %reload_ext gradio\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gradio Blocks instance: 1 backend functions\n",
              "-------------------------------------------\n",
              "fn_index=0\n",
              " inputs:\n",
              " |-<gradio.components.textbox.Textbox object at 0x7aa5769df410>\n",
              " outputs:\n",
              " |-<gradio.components.video.Video object at 0x7aa47a2c9210>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7aa5769ca590>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "%load_ext gradio\n",
        "\n",
        "# %%blocks\n",
        "import gradio as gr\n",
        "\n",
        "def qa_pipeline(question):\n",
        "    vid, s, e, ans = answer_query(question)\n",
        "    clip = extract_clip(vid, s, e, output_path=f\"clip_{vid}_{int(s)}_{int(e)}.mp4\")\n",
        "    return clip, ans\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Chat with Your Video Library\")\n",
        "    inp = gr.Textbox(label=\"Enter your question\")\n",
        "    vid_out = gr.Video(label=\"Relevant clip\")\n",
        "    text_out = gr.Textbox(label=\"Answer sentence\")\n",
        "    inp.submit(qa_pipeline, [inp], [vid_out, text_out])\n",
        "\n",
        "demo\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b1ccd2540224e0c9c49e37c28cd1bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee8a0c0b1de046fd9773b1b4b615e34f",
              "IPY_MODEL_e0d0e54dfff947efbe9b069bb78b328e",
              "IPY_MODEL_fccd54773bd9482a81a27e202ef9a7ef"
            ],
            "layout": "IPY_MODEL_6cfaa2d5e28c493c8ba47ce83e408d92"
          }
        },
        "ee8a0c0b1de046fd9773b1b4b615e34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de82aa79fc1943bf868fc70bb1b333b4",
            "placeholder": "​",
            "style": "IPY_MODEL_570dad6717524b23a5145c8894dd686f",
            "value": "Computing widget examples:   0%"
          }
        },
        "e0d0e54dfff947efbe9b069bb78b328e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c766a5912999411c83a2da98d65dc973",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccbd8e9754e94a07a2c5bc3423f7446e",
            "value": 1
          }
        },
        "fccd54773bd9482a81a27e202ef9a7ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d50317a1ed40d982a38b26d4141d4e",
            "placeholder": "​",
            "style": "IPY_MODEL_4e8a1ebe6e8c4e528697f50050721c91",
            "value": " 0/1 [00:00&lt;?, ?example/s]"
          }
        },
        "6cfaa2d5e28c493c8ba47ce83e408d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "de82aa79fc1943bf868fc70bb1b333b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570dad6717524b23a5145c8894dd686f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c766a5912999411c83a2da98d65dc973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccbd8e9754e94a07a2c5bc3423f7446e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7d50317a1ed40d982a38b26d4141d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8a1ebe6e8c4e528697f50050721c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}